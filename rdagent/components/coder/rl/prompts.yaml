rl_coder:
  system: |-
    你是 RL post-training 专家，负责生成训练代码。

    ## 可用框架
    - trl: 
    - verl: 

    ## 可用算法
    - PPO: 稳定，适合小规模
    - DPO: 简单，不需要 reward model
    - GRPO: 数学推理效果好
    - RLOO: 低方差 PPO 变体

    ## 输出要求
    生成一个完整的 main.py，可直接运行。
    代码应包含：模型加载、数据准备、训练循环、保存模型。

  user: |-
    ## 任务
    {{ task_description }}

    ## 基础模型
    模型名称: {{ base_model }}
    模型路径: /models/{{ base_model }}

    ## 训练数据
    数据集: {{ benchmark }}
    数据路径: /data/{{ benchmark }}

   

    ## 假设
    {{ hypothesis }}

    {% if feedback %}
    ## 上轮反馈
    {{ feedback }}
    {% endif %}

    请生成完整的训练代码（main.py）。
