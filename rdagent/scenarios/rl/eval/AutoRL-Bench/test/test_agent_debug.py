#!/usr/bin/env python
"""
Agent è°ƒè¯•æµ‹è¯•è„šæœ¬

è°ƒè¯•æ–¹å¼:
1. å…ˆå¯åŠ¨ API æœåŠ¡ (å¦ä¸€ä¸ªç»ˆç«¯): python -m autorl_bench.server
2. è¿è¡Œæ­¤è„šæœ¬: python test/test_agent_debug.py --steps 3

æ—¥å¿—è¾“å‡º:
- workspace/gsm8k_xxx/llm_logs/  - LLM æ¯æ¬¡äº¤äº’çš„å®Œæ•´è®°å½•
- workspace/gsm8k_xxx/agent_history.json - Agent å†³ç­–å†å²
"""

import os
import sys
import argparse

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["OPENAI_API_KEY"] = "sk-1234"
os.environ["OPENAI_API_BASE"] = "http://10.150.240.117:38889/v1"
os.environ["LITELLM_CHAT_MODEL"] = "gpt-4o"


def test_llm_only():
    """æµ‹è¯• LLM æ˜¯å¦å¯ç”¨"""
    print("=" * 50)
    print("æµ‹è¯• 1: LLM è¿æ¥")
    print("=" * 50)
    
    from agent.llm import LLMClient
    
    client = LLMClient(model="gpt-4o")
    response = client.chat([
        {"role": "user", "content": "1+1=? åªå›ç­”æ•°å­—"}
    ])
    print(f"LLM å“åº”: {response}")
    print("âœ… LLM è¿æ¥æ­£å¸¸\n")


def test_api_only():
    """æµ‹è¯• API æ˜¯å¦å¯ç”¨"""
    print("=" * 50)
    print("æµ‹è¯• 2: AutoRL-Bench API")
    print("=" * 50)
    
    import requests
    
    try:
        resp = requests.get("http://localhost:8000/scenarios/gsm8k", timeout=5)
        if resp.ok:
            data = resp.json()
            print(f"Scenario ID: {data.get('id')}")
            print(f"Baseline: {data.get('baseline_score')}")
            print("âœ… API æ­£å¸¸\n")
        else:
            print(f"âŒ API è¿”å›é”™è¯¯: {resp.status_code}")
            return False
    except Exception as e:
        print(f"âŒ API è¿æ¥å¤±è´¥: {e}")
        print("è¯·å…ˆå¯åŠ¨ API æœåŠ¡: python -m autorl_bench.server")
        return False
    
    return True


def test_agent(max_steps: int = 3):
    """æµ‹è¯• Agent"""
    print("=" * 50)
    print(f"æµ‹è¯• 3: Agent (æœ€å¤§ {max_steps} æ­¥)")
    print("=" * 50)
    
    from agent.simple_agent import run_agent
    
    result = run_agent(
        scenario_id="gsm8k",
        max_steps=max_steps,
        api_base="http://localhost:8000"
    )
    
    print("\n" + "=" * 50)
    print("Agent è¿è¡Œç»“æœ:")
    print("=" * 50)
    print(f"æ€»æ­¥æ•°: {result.get('total_steps')}")
    print(f"æ˜¯å¦æäº¤: {result.get('submitted')}")
    print(f"æ¨¡å‹è·¯å¾„: {result.get('final_model_path')}")
    print(f"å·¥ä½œç›®å½•: {result.get('workspace')}")
    
    # æç¤ºæŸ¥çœ‹æ—¥å¿—
    workspace = result.get('workspace')
    if workspace:
        print(f"\nğŸ“ æ—¥å¿—æ–‡ä»¶:")
        print(f"   LLM äº¤äº’: {workspace}/llm_logs/")
        print(f"   å†³ç­–å†å²: {workspace}/agent_history.json")


def main():
    parser = argparse.ArgumentParser(description="Agent è°ƒè¯•æµ‹è¯•")
    parser.add_argument("--steps", type=int, default=3, help="Agent æœ€å¤§æ­¥æ•°")
    parser.add_argument("--llm-only", action="store_true", help="åªæµ‹è¯• LLM")
    parser.add_argument("--api-only", action="store_true", help="åªæµ‹è¯• API")
    args = parser.parse_args()
    
    try:
        # æµ‹è¯• LLM
        test_llm_only()
        
        if args.llm_only:
            return
        
        # æµ‹è¯• API
        if not test_api_only():
            return
        
        if args.api_only:
            return
        
        # æµ‹è¯• Agent
        test_agent(args.steps)
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()

