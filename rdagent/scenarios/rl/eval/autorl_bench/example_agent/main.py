#!/usr/bin/env python
"""
Example Agent: è®­ç»ƒ+è¯„æµ‹ä¸€ä½“ï¼Œåœ¨ Docker é‡Œè¿è¡Œï¼ˆä¸è°ƒå¤–éƒ¨ APIï¼‰

ç”¨é€”ï¼šéªŒè¯ RL è®­ç»ƒ+è¯„æµ‹æµç¨‹æ˜¯å¦æ­£å¸¸ï¼ˆä¸ä¾èµ– LLM ç”Ÿæˆä»£ç ï¼‰

Usage 1 - ä½¿ç”¨å…¨å±€å˜é‡ï¼ˆæ¨èï¼Œä¸ loop.py ä¸€è‡´ï¼‰:
    # å…ˆè®¾ç½®ç¯å¢ƒå˜é‡æˆ–ç”¨ loop.py çš„å‚æ•°
    python rdagent/scenarios/rl/eval/autorl_bench/example_agent/main.py

Usage 2 - ä¼ å‚æ•°è¦†ç›–:
    python rdagent/scenarios/rl/eval/autorl_bench/example_agent/main.py \
        --base-model Qwen/Qwen2.5-Coder-0.5B-Instruct \
        --benchmark gsm8k \
        --train-ratio 0.01 \
        --eval-limit 50
"""

print("[DEBUG] Script starting...", flush=True)

import argparse
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
PROJECT_ROOT = Path(__file__).resolve().parent.parent.parent.parent.parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
print(f"[DEBUG] PROJECT_ROOT: {PROJECT_ROOT}", flush=True)

from rdagent.log import rdagent_logger as logger
from rdagent.scenarios.rl.env.conf import get_rl_env, RL_MODELS_DIR, RL_DATA_DIR
from rdagent.scenarios.rl.experiment.workspace import RLWorkspace


# è®­ç»ƒ+è¯„æµ‹ä¸€ä½“çš„ä»£ç ï¼ˆåœ¨ Docker å†…è¿è¡Œï¼‰
TRAIN_EVAL_CODE = '''
"""
GRPO Training + Evaluation for GSM8K (Docker å†…è¿è¡Œï¼Œä¸è°ƒ API)
"""

import json
import re
import os
import time

print("=" * 60, flush=True)
print("ğŸ³ Docker: GRPO Training + Evaluation", flush=True)
print("=" * 60, flush=True)

# ç¯å¢ƒå˜é‡
MODEL_PATH = os.environ.get("MODEL_PATH", "/models/Qwen2.5-Coder-0.5B-Instruct")
DATA_PATH = os.environ.get("DATA_PATH", "/data/gsm8k")
TRAIN_RATIO = float(os.environ.get("TRAIN_RATIO", "0.01"))
EVAL_LIMIT = int(os.environ.get("EVAL_LIMIT", "50"))
OUTPUT_DIR = "/workspace/output"

print(f"Model: {MODEL_PATH}", flush=True)
print(f"Data: {DATA_PATH}", flush=True)
print(f"Train Ratio: {TRAIN_RATIO * 100}%", flush=True)
print(f"Eval Limit: {EVAL_LIMIT}", flush=True)
print(f"Output: {OUTPUT_DIR}", flush=True)

# æ£€æŸ¥è·¯å¾„
import os.path
if not os.path.exists(MODEL_PATH):
    print(f" Model not found: {MODEL_PATH}", flush=True)
    exit(1)

train_file = f"{DATA_PATH}/train.jsonl"
test_file = f"{DATA_PATH}/test.jsonl"
if not os.path.exists(train_file):
    print(f" Train data not found: {train_file}", flush=True)
    exit(1)
if not os.path.exists(test_file):
    print(f" Test data not found: {test_file}", flush=True)
    exit(1)

print("\\n Paths verified", flush=True)

print("\\nLoading dependencies...", flush=True)
import torch
from datasets import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer


def extract_answer(text):
    """æå–æ•°å€¼ç­”æ¡ˆ"""
    match = re.search(r"####\\s*([-+]?\\d[\\d,]*\\.?\\d*)", text)
    if match:
        try:
            return float(match.group(1).replace(",", ""))
        except:
            pass
    numbers = re.findall(r"[-+]?\\d[\\d,]*\\.?\\d*", text)
    if numbers:
        try:
            return float(numbers[-1].replace(",", ""))
        except:
            pass
    return None


def load_data(file_path, ratio=1.0):
    """åŠ è½½æ•°æ®"""
    records = []
    with open(file_path, "r") as f:
        for line in f:
            item = json.loads(line)
            prompt = f"Solve this math problem step by step. Put your final answer after ####.\\n\\nQuestion: {item['question']}\\n\\nSolution:"
            records.append({
                "prompt": prompt,
                "question": item["question"],
                "answer": item["answer"],
            })
    if ratio < 1.0:
        n = max(10, int(len(records) * ratio))
        records = records[:n]
    return records


def gsm8k_reward_func(completions, answer, **kwargs):
    """GRPO reward: æ­£ç¡® +1, é”™è¯¯ -1"""
    rewards = []
    for completion, gold_answer in zip(completions, answer):
        pred = extract_answer(completion)
        gold = extract_answer(gold_answer)
        if pred is not None and gold is not None and abs(pred - gold) < 1e-6:
            rewards.append(1.0)
        else:
            rewards.append(-1.0)
    return rewards


# ============ Stage 1: Training ============
print("\\n" + "=" * 60, flush=True)
print("Stage 1: Training (GRPO)", flush=True)
print("=" * 60, flush=True)

print("\\nLoading training data...", flush=True)
train_data = load_data(train_file, TRAIN_RATIO)
print(f"Train samples: {len(train_data)}", flush=True)

dataset = Dataset.from_list([{"prompt": d["prompt"], "answer": d["answer"]} for d in train_data])

print("\\nLoading model...", flush=True)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

print("\\nConfiguring GRPO...", flush=True)
config = GRPOConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=1,
    learning_rate=1e-5,
    max_completion_length=256,
    num_generations=4,
    logging_steps=10,
    save_strategy="no",
    report_to="none",
    bf16=True,
)

trainer = GRPOTrainer(
    model=model,
    reward_funcs=gsm8k_reward_func,
    args=config,
    train_dataset=dataset,
    processing_class=tokenizer,
)

print("\\nStarting training...", flush=True)
start_time = time.time()
trainer.train()
train_time = time.time() - start_time

print(f"\\nSaving model to {OUTPUT_DIR}", flush=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
trainer.save_model(OUTPUT_DIR)
tokenizer.save_pretrained(OUTPUT_DIR)

print(f"\\n Training completed in {train_time:.1f}s", flush=True)

# é‡Šæ”¾å†…å­˜
del model, trainer
torch.cuda.empty_cache()


# ============ Stage 2: Evaluation ============
print("\\n" + "=" * 60, flush=True)
print("ğŸ“Š Stage 2: Evaluation", flush=True)
print("=" * 60, flush=True)

print("\\nLoading test data...", flush=True)
test_data = load_data(test_file)
if EVAL_LIMIT > 0:
    test_data = test_data[:EVAL_LIMIT]
print(f"Test samples: {len(test_data)}", flush=True)

print("\\nLoading trained model...", flush=True)
tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR, trust_remote_code=True)
tokenizer.padding_side = 'left'
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    OUTPUT_DIR,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

print("\\nRunning evaluation...", flush=True)
correct = 0
total = 0
start_time = time.time()

for i, item in enumerate(test_data):
    prompt = item["prompt"]
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            do_sample=False,
            pad_token_id=tokenizer.pad_token_id,
        )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    response = response[len(prompt):].strip()
    
    pred = extract_answer(response)
    gold = extract_answer(item["answer"])
    
    is_correct = pred is not None and gold is not None and abs(pred - gold) < 1e-6
    if is_correct:
        correct += 1
    total += 1
    
    if (i + 1) % 10 == 0:
        acc = correct / total * 100
        print(f"  [{i+1}/{len(test_data)}] Accuracy: {acc:.1f}% ({correct}/{total})", flush=True)

eval_time = time.time() - start_time
accuracy = correct / total * 100

print(f"\\n Evaluation completed in {eval_time:.1f}s", flush=True)
print(f" Final Accuracy: {accuracy:.2f}% ({correct}/{total})", flush=True)


# ============ Save Results ============
results = {
    "accuracy": round(accuracy, 2),
    "correct": correct,
    "total": total,
    "train_time": round(train_time, 1),
    "eval_time": round(eval_time, 1),
    "config": {
        "train_ratio": TRAIN_RATIO,
        "eval_limit": EVAL_LIMIT,
    }
}

result_file = f"{OUTPUT_DIR}/metrics.json"
with open(result_file, "w") as f:
    json.dump(results, f, indent=2)

print(f"\\nğŸ“ Results saved to: {result_file}", flush=True)
print("\\n" + "=" * 60, flush=True)
print("âœ… Done!", flush=True)
print("=" * 60, flush=True)
'''


def parse_args():
    """è§£æå‚æ•°ï¼Œæ”¯æŒä»å…¨å±€å˜é‡è¯»å–é»˜è®¤å€¼"""
    from rdagent.app.rl.conf import RL_RD_SETTING
    
    parser = argparse.ArgumentParser(description="Example Agent for RL Post-training (Docker)")
    parser.add_argument("--base-model", default=None, help="Base model name (default: RL_RD_SETTING.base_model)")
    parser.add_argument("--benchmark", default=None, help="Benchmark name (default: RL_RD_SETTING.benchmark)")
    parser.add_argument("--train-ratio", type=float, default=0.01, help="Training data ratio")
    parser.add_argument("--eval-limit", type=int, default=50, help="Evaluation sample limit")
    parser.add_argument("--timeout", type=int, default=3600, help="Training timeout (seconds)")
    
    args = parser.parse_args()
    
    # å¦‚æœæ²¡ä¼ å‚æ•°ï¼Œä»å…¨å±€å˜é‡è¯»å–
    if args.base_model is None:
        args.base_model = RL_RD_SETTING.base_model
    if args.benchmark is None:
        args.benchmark = RL_RD_SETTING.benchmark
    
    # æ£€æŸ¥å¿…éœ€å‚æ•°
    if not args.base_model:
        print("âŒ è¯·æŒ‡å®š --base-model æˆ–è®¾ç½® RL_RD_SETTING.base_model", flush=True)
        sys.exit(1)
    if not args.benchmark:
        print("âŒ è¯·æŒ‡å®š --benchmark æˆ–è®¾ç½® RL_RD_SETTING.benchmark", flush=True)
        sys.exit(1)
    
    return args


def main():
    args = parse_args()
    
    print("=" * 60, flush=True)
    print("ğŸ¤– Example Agent for RL Post-training (Docker)", flush=True)
    print("=" * 60, flush=True)
    print(f"  Base Model: {args.base_model}", flush=True)
    print(f"  Benchmark: {args.benchmark}", flush=True)
    print(f"  Train Ratio: {args.train_ratio * 100}%", flush=True)
    print(f"  Eval Limit: {args.eval_limit}", flush=True)
    print(f"  Timeout: {args.timeout}s", flush=True)
    
    # æ£€æŸ¥è·¯å¾„
    model_path = RL_MODELS_DIR / args.base_model
    data_path = RL_DATA_DIR / args.benchmark
    
    if not model_path.exists():
        print(f"âŒ Model not found: {model_path}", flush=True)
        sys.exit(1)
    if not data_path.exists():
        print(f"âŒ Data not found: {data_path}", flush=True)
        sys.exit(1)
    
    print(f"âœ… Model: {model_path}", flush=True)
    print(f"âœ… Data: {data_path}", flush=True)
    
    # åˆ›å»º workspace
    print("\nğŸ“ Creating workspace...", flush=True)
    workspace = RLWorkspace()
    workspace.inject_files(**{"main.py": TRAIN_EVAL_CODE})
    
    # è·å– Docker ç¯å¢ƒï¼ˆæ ¹æ® benchmark è‡ªåŠ¨é€‰æ‹©é•œåƒï¼‰
    print("\nğŸ³ Getting Docker environment...", flush=True)
    env = get_rl_env(benchmark=args.benchmark, timeout=args.timeout)
    
    # è®¾ç½®ç¯å¢ƒå˜é‡
    env_vars = {
        "MODEL_PATH": f"/models/{args.base_model}",
        "DATA_PATH": f"/data/{args.benchmark}",
        "TRAIN_RATIO": str(args.train_ratio),
        "EVAL_LIMIT": str(args.eval_limit),
    }
    
    # æ‰§è¡Œè®­ç»ƒ+è¯„æµ‹
    print("\nğŸš€ Starting training + evaluation in Docker...", flush=True)
    print(f"Env vars: {env_vars}", flush=True)
    
    workspace.prepare()
    workspace.inject_files(**workspace.file_dict)
    result = env.run(
        entry="python main.py",
        local_path=str(workspace.workspace_path),
        env=env_vars,
    )
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 60, flush=True)
    print("ğŸ“‹ Result", flush=True)
    print("=" * 60, flush=True)
    print(f"Exit code: {result.exit_code}", flush=True)
    print(f"Running time: {result.running_time:.2f}s", flush=True)
    
    if result.stdout:
        print("\n--- stdout ---", flush=True)
        # åªæ‰“å°æœ€å 5000 å­—ç¬¦
        stdout = result.stdout
        if len(stdout) > 5000:
            print("... (truncated) ...", flush=True)
            stdout = stdout[-5000:]
        print(stdout, flush=True)
    
    if result.exit_code != 0:
        print("âŒ Failed!", flush=True)
        sys.exit(1)
    
    print("\nâœ… Done!", flush=True)


if __name__ == "__main__":
    main()
