{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118759dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78505eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627632f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48c9f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ijson\n",
    "\n",
    "def load_first_n_json(path, n):\n",
    "    data = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        objects = ijson.items(f, \"item\")\n",
    "        for i, obj in enumerate(objects):\n",
    "            if i >= n:\n",
    "                break\n",
    "            data.append(obj)\n",
    "    return data\n",
    "\n",
    "pairs = load_first_n_json(\"/data/userdata/v-lijingyuan/dpo/final_data.json\", 10000)\n",
    "print(len(pairs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3de74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "# winner_lens = [len(p[\"winner\"]) for p in pairs]\n",
    "# loser_lens = [len(p[\"loser\"]) for p in pairs]\n",
    "\n",
    "# print(\"Winner max length:\", max(winner_lens))\n",
    "# print(\"Loser max length:\", max(loser_lens))\n",
    "\n",
    "# # 统计长度分布（出现次数）\n",
    "# winner_len_counter = Counter(winner_lens)\n",
    "# loser_len_counter = Counter(loser_lens)\n",
    "\n",
    "# print(\"Winner length distribution:\", winner_len_counter)\n",
    "# print(\"Loser length distribution:\", loser_len_counter)\n",
    "\n",
    "# # 可视化（直方图）\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.hist(winner_lens, bins=30, alpha=0.5, label=\"winner\")\n",
    "# plt.hist(loser_lens, bins=30, alpha=0.5, label=\"loser\")\n",
    "# plt.xlabel(\"Sequence length\")\n",
    "# plt.ylabel(\"Count\")\n",
    "# plt.title(\"Winner/Loser Length Distribution\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6aa392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 读取数据\n",
    "with open(\"/data/userdata/v-lijingyuan/dpo/final_pairs.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Loaded {len(data)} data points for inference.\")\n",
    "\n",
    "def get_parent_hypotheses(id_to_entry, entry):\n",
    "    \"\"\"\n",
    "    递归向上查找所有父条目（parent_id 链），\n",
    "    仅保留其中 feedback_decision == True 的 hypothesis。\n",
    "    \"\"\"\n",
    "    hypotheses = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "\n",
    "    visited = set()  # 防止循环引用\n",
    "    while parent_id and parent_id not in visited:\n",
    "        visited.add(parent_id)\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "\n",
    "        if parent_entry is None:\n",
    "            break  # 没找到父节点则停止\n",
    "\n",
    "        parent_input = parent_entry.get('input', {})\n",
    "        if parent_input.get('feedback_decision') is True:\n",
    "            hypotheses.append(parent_input.get('hypothesis_chain'))\n",
    "\n",
    "        parent_id = parent_input.get('parent_id')  # 继续往上找\n",
    "\n",
    "    return hypotheses\n",
    "\n",
    "# 只保留目标任务的条目\n",
    "entries = [d for d in data if d[\"input\"].get(\"comptation_name\") == \"tweet-sentiment-extraction\"]\n",
    "print(f\"Filtered to {len(entries)} tweet-sentiment-extraction entries.\")\n",
    "\n",
    "# 构建 id -> entry 的映射，用于追踪父节点\n",
    "id_to_entry = {\n",
    "    f\"{d['input']['exp_name']} {d['input']['comptation_name']} {d['input']['loop_id']}\": d\n",
    "    for d in entries\n",
    "}\n",
    "\n",
    "# 为每个 entry 提取父 hypothesis 链\n",
    "for e in entries:\n",
    "    parent_hyps = get_parent_hypotheses(id_to_entry, e)\n",
    "    e[\"parent_hypotheses\"] = parent_hyps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2572f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c7427",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"/home/bowen/workspace/fine-tune/amlt_jsons_new/needed-killdeer.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eae6999",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_hyps = [\n",
    "    \"Try logistic regression with standard scaling\",\n",
    "    \"Switch to XGBoost with tuned learning rate\",\n",
    "    \"Add cross-validation and early stopping\"\n",
    "]\n",
    "\n",
    "current_hyp = \"Stack a transformer encoder on top of tabular embeddings and tune via SWA\"\n",
    "max_tokens = 35  # 设很小，方便演示\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb3b7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_hypo_chain(parent_hyps = parent_hyps, current_hyp = current_hyp, max_tokens = max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9249f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hypo_chain(parent_hyps, current_hyp, max_tokens=9000):\n",
    "    \"\"\"\n",
    "    拼接 parent_hyps （按时间顺序），加入 current_hyp，截断到 max_tokens\n",
    "    \"\"\"\n",
    "    sep = \"->\"\n",
    "    chain = parent_hyps[::-1] + [current_hyp]  # parent从早到晚 + 当前\n",
    "\n",
    "    while True:\n",
    "        chain_text = sep.join(chain)\n",
    "        tokenized = tokenizer(chain_text, add_special_tokens=False)\n",
    "        tokens = len(tokenized[\"input_ids\"])\n",
    "\n",
    "        # ✅ 已满足长度\n",
    "        if tokens <= max_tokens:\n",
    "            return chain_text\n",
    "\n",
    "        # ✅ 截断最前面的 parent hypothesis\n",
    "        if len(chain) > 1:\n",
    "            chain.pop(0)\n",
    "            continue\n",
    "\n",
    "        # ✅ 到这说明只剩 current_hyp 还超 → 硬截断 current_hyp 尾部\n",
    "        truncated_ids = tokenized[\"input_ids\"][-max_tokens:]\n",
    "        return tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_parent_hypotheses(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    hypotheses = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            hypotheses.append(parent_entry['input']['hypothesis'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return hypotheses\n",
    "\n",
    "\n",
    "def get_parent_scores(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    scores = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            scores.append(parent_entry['input']['valid_score'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e7c41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "final_pairs = []\n",
    "amlt_json_path = Path(\"/home/bowen/workspace/fine-tune/amlt_jsons\")\n",
    "for exp_json in amlt_json_path.iterdir():\n",
    "    with open(exp_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for ids, loop_data in data.items():\n",
    "        comptation_name = ids.split(\" \")[1]\n",
    "        if ids.split(\" \")[-1]== \"scenario\":\n",
    "            bigger_is_better = int(loop_data['metric_direction'])\n",
    "\n",
    "        if \"final_hypothesis\" in loop_data and \"feedback\" in loop_data and \"code\" in loop_data:\n",
    "            first_metric = next(iter(loop_data[\"valid_score\"].values()))\n",
    "            alpaca_data = {\n",
    "                    \"input\": {\n",
    "                        \"exp_name\": exp_json.name.replace(\".json\", \"\"),\n",
    "                        \"comptation_name\":comptation_name,\n",
    "                        \"bigger_is_better\": bigger_is_better,\n",
    "                        \"loop_id\": int(ids.split(\" \")[-1]),\n",
    "                        \"hypothesis\": loop_data[\"final_hypothesis\"][\"hypothesis\"],\n",
    "                        #\"test_report\": loop_data[\"test_report\"][\"score\"],\n",
    "                        \"valid_score\": first_metric.get(\"ensemble\", None),\n",
    "                        \"feedback_decision\": loop_data[\"feedback\"]['decision'],\n",
    "                        \"parent_id\": loop_data.get(\"parent_id\", None) \n",
    "                    }\n",
    "                }\n",
    "            all_pairs.append(alpaca_data)\n",
    "\n",
    "    all_pairs_new = []\n",
    "    id_to_entry = {}\n",
    "    for entry in all_pairs:\n",
    "        key = f\"{entry['input']['exp_name']} {entry['input']['comptation_name']} {entry['input']['loop_id']}\"\n",
    "        id_to_entry[key] = entry\n",
    "    for target_entry in all_pairs:\n",
    "        parent_hyps = get_parent_hypotheses(id_to_entry,target_entry)\n",
    "        target_entry['input']['hypothesis_chain'] =build_hypo_chain(parent_hyps, target_entry['input'].get('hypothesis'))#\"<think_step>\".join(parent_hyps[::-1] + [target_entry['input'].get('hypothesis')] )\n",
    "        parnet_scores = get_parent_scores(id_to_entry,target_entry)\n",
    "        if len(parnet_scores)>0: \n",
    "            target_entry['input']['parent_score'] = parnet_scores[0]\n",
    "        else:\n",
    "            target_entry['input']['parent_score'] = 10000000\n",
    "\n",
    "        all_pairs_new.append(target_entry)\n",
    "    del all_pairs\n",
    "    final_pairs.extend(all_pairs_new)\n",
    "\n",
    "    preference_pairs = []\n",
    "\n",
    "    # --- 第一步：按比赛名分组 ---\n",
    "    groups = defaultdict(list)\n",
    "    for item in all_pairs_new:\n",
    "        inp = item[\"input\"]\n",
    "        comp_name = inp[\"comptation_name\"]\n",
    "        groups[comp_name].append(inp)\n",
    "\n",
    "    # --- 第二步：在每个比赛内生成 C(n, 2) 偏好对 ---\n",
    "    for comp_name, items in groups.items():\n",
    "        if len(items) < 2:\n",
    "            continue  # 跳过不足两条的比赛\n",
    "        \n",
    "        bigger_is_better = items[0][\"bigger_is_better\"]\n",
    "\n",
    "        for a, b in combinations(items, 2):\n",
    "            score_a = a[\"valid_score\"] - a[\"parent_score\"]\n",
    "            score_b = b[\"valid_score\"] - a[\"parent_score\"]\n",
    "\n",
    "            # 判断优劣关系\n",
    "            if bigger_is_better == 1:\n",
    "                winner, loser = (a, b) if score_a > score_b else (b, a)\n",
    "            else:\n",
    "                winner, loser = (a, b) if score_a < score_b else (b, a)\n",
    "            if score_a >-10000:\n",
    "                preference_pairs.append({\n",
    "                    \"comptation_name\": comp_name,\n",
    "                    \"loop_pair\": (a[\"loop_id\"], b[\"loop_id\"]),\n",
    "                    \"winner\": winner[\"hypothesis_chain\"],\n",
    "                    \"loser\": loser[\"hypothesis_chain\"],\n",
    "    #               \"score_diff\": abs(score_a - score_b),  # 可选：分数差\n",
    "                })\n",
    "    final_data.extend(preference_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bab718",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be29b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34bc17af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"final_data_diff.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"final_pairs_diff.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd0cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp -f /data/userdata/v-lijingyuan/dpo/final_data.json /home/bowen/workspace/fine-tune/custom/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b949017",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "final_pairs = []\n",
    "\n",
    "for exp_json in amlt_json_path.iterdir():\n",
    "    with open(exp_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "\n",
    "    all_pairs = []\n",
    "    for ids, loop_data in data.items():\n",
    "        comptation_name = ids.split(\" \")[1]\n",
    "        if ids.split(\" \")[-1]== \"scenario\":\n",
    "            bigger_is_better = int(loop_data['metric_direction'])\n",
    "\n",
    "        if \"final_hypothesis\" in loop_data and \"feedback\" in loop_data and \"code\" in loop_data and \"sota_hypothesis\" in loop_data:\n",
    "            first_metric = next(iter(loop_data[\"valid_score\"].values()))\n",
    "            alpaca_data = {\n",
    "                    \"input\": {\n",
    "                        \"comptation_name\":comptation_name,\n",
    "                        \"bigger_is_better\": bigger_is_better,\n",
    "                        \"loop_id\": int(ids.split(\" \")[-1]),\n",
    "                        \"hypothesis\": loop_data[\"final_hypothesis\"][\"hypothesis\"] + \"currnet sota hypothesis is \" + loop_data[\"sota_hypothesis\"][\"hypothesis\"],\n",
    "                        \"test_report\": loop_data[\"test_report\"][\"score\"],\n",
    "                        \"valid_score\": first_metric.get(\"ensemble\", None),\n",
    "                        \n",
    "                        \"feedback_decision\": loop_data[\"feedback\"]['decision']\n",
    "                    }\n",
    "                }\n",
    "            all_pairs.append(alpaca_data)\n",
    "    final_pairs.extend(all_pairs)\n",
    "\n",
    "    \n",
    "    preference_pairs = []\n",
    "\n",
    "    # --- 第一步：按比赛名分组 ---\n",
    "    groups = defaultdict(list)\n",
    "    for item in all_pairs:\n",
    "        inp = item[\"input\"]\n",
    "        comp_name = inp[\"comptation_name\"]\n",
    "        groups[comp_name].append(inp)\n",
    "\n",
    "    # --- 第二步：在每个比赛内生成 C(n, 2) 偏好对 ---\n",
    "    for comp_name, items in groups.items():\n",
    "        if len(items) < 2:\n",
    "            continue  # 跳过不足两条的比赛\n",
    "        \n",
    "        bigger_is_better = items[0][\"bigger_is_better\"]\n",
    "\n",
    "        for a, b in combinations(items, 2):\n",
    "            score_a = a[\"valid_score\"]\n",
    "            score_b = b[\"valid_score\"]\n",
    "\n",
    "            # 判断优劣关系\n",
    "            if bigger_is_better == 1:\n",
    "                winner, loser = (a, b) if score_a > score_b else (b, a)\n",
    "            else:\n",
    "                winner, loser = (a, b) if score_a < score_b else (b, a)\n",
    "\n",
    "            preference_pairs.append({\n",
    "                \"comptation_name\": comp_name,\n",
    "                \"loop_pair\": (a[\"loop_id\"], b[\"loop_id\"]),\n",
    "                \"winner\": winner[\"hypothesis\"],\n",
    "                \"loser\": loser[\"hypothesis\"],\n",
    "                \"score_diff\": abs(score_a - score_b),  # 可选：分数差\n",
    "            })\n",
    "    final_data.extend(preference_pairs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
