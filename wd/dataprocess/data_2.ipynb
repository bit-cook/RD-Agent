{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3dd655d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff2bd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6ff4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "amlt_root_json_path = Path(\"/home/bowen/workspace/fine-tune/root_id_map.json\")\n",
    "#for exp_json in amlt_json_path.iterdir()[0:3]:\n",
    "with open(amlt_root_json_path, \"r\") as f:\n",
    "    root_map = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f537896",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f698198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hypo_chain(parent_hyps, current_hyp, max_tokens=8000):\n",
    "    \"\"\"\n",
    "    拼接 parent_hyps （按时间顺序），加入 current_hyp，截断到 max_tokens\n",
    "    \"\"\"\n",
    "    sep = \"->\"\n",
    "    chain = parent_hyps[::-1] + [current_hyp]  # parent从早到晚 + 当前\n",
    "\n",
    "    while True:\n",
    "        chain_text = sep.join(chain)\n",
    "        tokenized = tokenizer(chain_text, add_special_tokens=False)\n",
    "        tokens = len(tokenized[\"input_ids\"])\n",
    "\n",
    "        # ✅ 已满足长度\n",
    "        if tokens <= max_tokens:\n",
    "            return chain_text\n",
    "\n",
    "        # ✅ 截断最前面的 parent hypothesis\n",
    "        if len(chain) > 1:\n",
    "            chain.pop(0)\n",
    "            continue\n",
    "\n",
    "        # ✅ 到这说明只剩 current_hyp 还超 → 硬截断 current_hyp 尾部\n",
    "        truncated_ids = tokenized[\"input_ids\"][-max_tokens:]\n",
    "        return tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_parent_hypotheses(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    hypotheses = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            hypotheses.append(parent_entry['input']['hypothesis'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return hypotheses\n",
    "\n",
    "\n",
    "def get_parent_scores(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    scores = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            scores.append(parent_entry['input']['valid_score'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "final_pairs = []\n",
    "amlt_json_path = Path(\"/home/bowen/workspace/fine-tune/amlt_jsons\")\n",
    "#for exp_json in amlt_json_path.iterdir()[0:3]:\n",
    "for exp_json in list(amlt_json_path.iterdir()):\n",
    "    with open(exp_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for ids, loop_data in data.items():\n",
    "        comptation_name = ids.split(\" \")[1]\n",
    "        if ids.split(\" \")[-1]== \"scenario\":\n",
    "            bigger_is_better = int(loop_data['metric_direction'])\n",
    "\n",
    "        if \"final_hypothesis\" in loop_data and \"feedback\" in loop_data and \"code\" in loop_data:\n",
    "            first_metric = next(iter(loop_data[\"valid_score\"].values()))\n",
    "            alpaca_data = {\n",
    "                    \"input\": {\n",
    "                        \"exp_name\": exp_json.name.replace(\".json\", \"\"),\n",
    "                        \"comptation_name\":comptation_name,\n",
    "                        \"bigger_is_better\": bigger_is_better,\n",
    "                        \"loop_id\": int(ids.split(\" \")[-1]),\n",
    "                        \"hypothesis\": loop_data[\"final_hypothesis\"][\"hypothesis\"],\n",
    "                        #\"test_report\": loop_data[\"test_report\"][\"score\"],\n",
    "                        \"valid_score\": first_metric.get(\"ensemble\", None),\n",
    "                        \"feedback_decision\": loop_data[\"feedback\"]['decision'],\n",
    "                        \"parent_id\": loop_data.get(\"parent_id\", None) ,\n",
    "                        \"root_id\" : int(root_map[ids].split(\" \")[-1]) if ids in root_map else None\n",
    "                    }\n",
    "                }\n",
    "            all_pairs.append(alpaca_data)\n",
    "\n",
    "    all_pairs_new = []\n",
    "    id_to_entry = {}\n",
    "    for entry in all_pairs:\n",
    "        key = f\"{entry['input']['exp_name']} {entry['input']['comptation_name']} {entry['input']['loop_id']}\"\n",
    "        id_to_entry[key] = entry\n",
    "\n",
    "    id_to_entry1 = {}\n",
    "    for entry in all_pairs:\n",
    "        key = (entry['input']['exp_name'], entry['input']['comptation_name'], int(entry['input']['loop_id']))\n",
    "        id_to_entry1[key] = entry\n",
    "\n",
    "\n",
    "    for target_entry in all_pairs:\n",
    "        parent_hyps = get_parent_hypotheses(id_to_entry,target_entry)\n",
    "        target_entry['input']['hypothesis_chain'] =build_hypo_chain(parent_hyps, target_entry['input'].get('hypothesis'))#\"<think_step>\".join(parent_hyps[::-1] + [target_entry['input'].get('hypothesis')] )\n",
    "        parnet_scores = get_parent_scores(id_to_entry,target_entry)\n",
    "        if len(parnet_scores)>0: \n",
    "            target_entry['input']['parent_score'] = parnet_scores[0]\n",
    "        else:\n",
    "            target_entry['input']['parent_score'] = 10000000\n",
    "\n",
    "        all_pairs_new.append(target_entry)\n",
    "\n",
    "    del all_pairs\n",
    "    final_pairs.extend(all_pairs_new)\n",
    "\n",
    "\n",
    "    preference_pairs = []\n",
    "\n",
    "    # --- 第一步：按比赛名分组 ---\n",
    "\n",
    "\n",
    "    # --- 第二步：按 (exp_name, comptation_name, root_id) 分组 ---\n",
    "    groups = defaultdict(list)\n",
    "    for item in all_pairs_new:\n",
    "        inp = item[\"input\"]\n",
    "        key = (inp[\"exp_name\"], inp[\"comptation_name\"], inp[\"root_id\"])\n",
    "        groups[key].append(inp)\n",
    "\n",
    "\n",
    "    # --- 第三步：在每个组内生成 C(n, 2) 偏好对 ---\n",
    "    for (exp_name, comp_name, root_id), items in groups.items():\n",
    "        if len(items) < 2:\n",
    "            continue  # 轨道上少于两条样本就跳过\n",
    "\n",
    "        bigger_is_better = items[0][\"bigger_is_better\"]\n",
    "\n",
    "        for a, b in combinations(items, 2):\n",
    "            score_a = a[\"valid_score\"] - a[\"parent_score\"]\n",
    "            score_b = b[\"valid_score\"] - b[\"parent_score\"]\n",
    "\n",
    "            if bigger_is_better == 1:\n",
    "                winner, loser = (a, b) if score_a > score_b else (b, a)\n",
    "            else:\n",
    "                winner, loser = (a, b) if score_a < score_b else (b, a)\n",
    "\n",
    "\n",
    "            #condition1 = (a[\"loop_id\"]- b[\"loop_id\"])<10\n",
    "            condition2 = max(len(a[\"hypothesis_chain\"].split(\"->\")) , len(b[\"hypothesis_chain\"].split(\"->\"))) <6\n",
    "\n",
    "\n",
    "            # tokenized1 = tokenizer( a[\"hypothesis_chain\"], add_special_tokens=False)\n",
    "            # tokens1 = len(tokenized1[\"input_ids\"])\n",
    "\n",
    "            # tokenized2 = tokenizer(b[\"hypothesis_chain\"], add_special_tokens=False)\n",
    "            # tokens2 = len(tokenized2[\"input_ids\"])\n",
    "\n",
    "\n",
    "            #condition1 = (tokens1<2000) and (tokens2<2000)\n",
    "\n",
    "\n",
    "            if score_a > -10000 and condition2:#condition1 and condition2:\n",
    "                preference_pairs.append({\n",
    "                    \"exp_name\": exp_name,\n",
    "                    \"comptation_name\": comp_name,\n",
    "                    \"root_id\": root_id,\n",
    "                    \"loop_pair\": (a[\"loop_id\"], b[\"loop_id\"]),\n",
    "                    \"winner\": winner[\"hypothesis_chain\"],\n",
    "                    \"loser\": loser[\"hypothesis_chain\"],\n",
    "                    \"score_a\":score_a,\n",
    "                    \"score_b\":score_b,\n",
    "                })\n",
    "\n",
    "    final_data.extend(preference_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e02ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pairs = [k for k in final_pairs if len(k[\"input\"][\"hypothesis_chain\"].split(\"->\"))<6 and len(tokenizer(k[\"input\"][\"hypothesis_chain\"], add_special_tokens=False)[\"input_ids\"]) < 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659bc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471742c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "S = []\n",
    "for i in range(len(final_pairs)):\n",
    "    chain_text = final_pairs[i][\"input\"][\"hypothesis_chain\"]\n",
    "    tokenized = tokenizer(chain_text, add_special_tokens=False)\n",
    "    tokens = len(tokenized[\"input_ids\"])\n",
    "    S.append(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e800e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(S, bins=30)\n",
    "plt.xlabel(\"Token length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of tokenized hypothesis_chain length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4b744e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#less than 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a503c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae578aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = [k for k in final_data if len(tokenizer(k[\"winner\"], add_special_tokens=False)[\"input_ids\"]) < 2000 and len(tokenizer(k[\"loser\"], add_special_tokens=False)[\"input_ids\"]) < 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d211c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48ff0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"final_data_diff_2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"final_pairs_diff_2.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(final_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b6e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "scp -r -B ib.ep10.213428.xyz /data/userdata/v-lijingyuan/ckpt/rm_bt_s1028_gc ib.ep03.213428.xyz:/data/userdata/v-lijingyuan \n",
    "\n",
    "\n",
    "scp -r -B /data/userdata/v-lijingyuan/ckpt/rm_bt_s1028_gc \\\n",
    "      username@ib.ep03.213428.xyz:/data/userdata/v-lijingyuan/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
