{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3074dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# case study \n",
    "# exp baseline artistic-toad  : powerful-crow type 2    6个比赛4个比基线高 curious-cattle type 1    6个比赛5个比基线高 \n",
    "import pandas as pd \n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d15b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = \"smooth-tuna\"\n",
    "\n",
    "wo = \"quality-cougar\"\n",
    "EXP = [wo,baseline]\n",
    "\n",
    "\n",
    "EVO = [ \"cassava-leaf-disease-classification\",\n",
    "            \"h-and-m-personalized-fashion-recommendations\",\n",
    "            \"jigsaw-toxic-comment-classification-challenge\",\n",
    "            \"leaf-classification\",\n",
    "            \"tweet-sentiment-extraction\",\n",
    "            \"us-patent-phrase-to-phrase-matching\",\n",
    "            \"whale-categorization-playground\",\n",
    "            \"learning-agency-lab-automated-essay-scoring-2\",\n",
    "            \"aptos2019-blindness-detection\",\n",
    "            \"kuzushiji-recognition\",\n",
    "            \"herbarium-2020-fgvc7\",\n",
    "            \"text-normalization-challenge-russian-language\",\n",
    "            \"rsna-miccai-brain-tumor-radiogenomic-classification\",\n",
    "            \"freesound-audio-tagging-2019\",\n",
    "            \"mlsp-2013-birds\",\n",
    "            \"spooky-author-identification\",\n",
    "            \"hubmap-kidney-segmentation\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09379c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = []\n",
    "\n",
    "for exp in EXP:\n",
    "    p0= f'/data/share_folder_local/amlt/{exp}/combined_logs/summary.pkl'\n",
    "    with open(p0, 'rb') as f:\n",
    "        summary  = pickle.load(f)\n",
    "    for idx, competition in enumerate(summary.keys()):\n",
    "        if competition.split(\".\")[-2] in EVO:\n",
    "            data = summary[competition]\n",
    "            print(f\"Experiment: {exp}, Competition: {competition}\")\n",
    "            test_scores_df = pd.DataFrame.from_dict(data[\"test_scores\"], orient=\"index\", columns=[\"Test Score\"])\n",
    "            test_scores_df.index.name = \"Loop\"\n",
    "            \n",
    "            # valid_scores_dict = data[\"valid_scores\"]\n",
    "            # # 提取 ensemble 验证分数\n",
    "            # ensemble_scores = {}\n",
    "            # for loop_id, df in valid_scores_dict.items():\n",
    "            #     if \"ensemble\" in df.index:\n",
    "            #         ensemble_scores[loop_id] = df.loc[\"ensemble\"].iloc[0]\n",
    "            ensemble_scores = test_scores_df[\"Test Score\"]\n",
    "            #print(ensemble_scores)\n",
    "            bronze_threshold = data[\"bronze_threshold\"]\n",
    "            silver_threshold = data[\"silver_threshold\"]\n",
    "            if ensemble_scores.size > 0:\n",
    "                if bronze_threshold > silver_threshold:\n",
    "                    F.append((exp, competition, ensemble_scores.min(), -1))\n",
    "                else:\n",
    "                    F.append((exp, competition, ensemble_scores.max(), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [\n",
    "    item for item in F\n",
    "    if item[1].endswith(('.1', '.2', '.3'))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaf804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7578ce8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "data = F  # F 内部为 (exp, comp, score, direction)\n",
    "\n",
    "def strip_version(name):\n",
    "    if \".\" in name:\n",
    "        return name.rsplit(\".\", 1)[0]\n",
    "    return name\n",
    "\n",
    "# 第一步：按 competition + experiment 聚合所有 score\n",
    "comp2scores_raw = defaultdict(lambda: defaultdict(list))\n",
    "comp_direction = {}  # 每个 competition 的方向，只需要记录一次\n",
    "\n",
    "for exp, comp, score, direction in data:\n",
    "    comp_base = strip_version(comp)\n",
    "    comp2scores_raw[comp_base][exp].append(score)\n",
    "    comp_direction[comp_base] = direction\n",
    "\n",
    "# 第二步：求平均值\n",
    "comp2scores = defaultdict(dict)\n",
    "for comp, exp_dict in comp2scores_raw.items():\n",
    "    direction = comp_direction[comp]   # 1 or -1\n",
    "    for exp, score_list in exp_dict.items():\n",
    "        comp2scores[comp][exp] = (np.mean(score_list),np.std(score_list))\n",
    "        # if direction == 1:\n",
    "        #     comp2scores[comp][exp] = np.mean(score_list)\n",
    "        # else:\n",
    "        #     comp2scores[comp][exp] = np.mean(score_list)\n",
    "\n",
    "# target = \"curious-cattle\"\n",
    "\n",
    "# for comp, score_dict in comp2scores.items():\n",
    "\n",
    "#     if target not in score_dict:\n",
    "#         continue\n",
    "\n",
    "#     direction = comp_direction[comp]  #\n",
    "#     base = score_dict[target]\n",
    "\n",
    "    # diffs = {exp: (score - base) * direction for exp, score in score_dict.items()}\n",
    "\n",
    "    # sorted_exps = sorted(diffs, key=diffs.get, reverse=True)\n",
    "    # sorted_vals = [diffs[e] for e in sorted_exps]\n",
    "\n",
    "    # plt.figure(figsize=(5, 4))\n",
    "    # plt.barh(sorted_exps, sorted_vals)\n",
    "    # plt.axvline(0, color='black', linewidth=1)\n",
    "    # plt.title(f\"Difference from humble-cardinal — {comp}\")\n",
    "    # plt.xlabel(\"Performance Difference (positive = better)\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "    # plt.savefig(f'diff_{comp}.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6854dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "for comp, exp_dict in comp2scores.items():\n",
    "    for exp, (mean, std) in exp_dict.items():\n",
    "        rows.append({\n",
    "            \"competition\": comp,\n",
    "            \"experiment\": exp,\n",
    "            \"mean\": mean,\n",
    "            \"std\": std,\n",
    "        })\n",
    "\n",
    "df_base = pd.DataFrame(rows)\n",
    "print(df_base.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42815d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0b58d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def build_sota_df(run_dirs, evo_list, experiment_name=\"rd-agent-sota\"):\n",
    "    rows = []\n",
    "\n",
    "    for run_dir in run_dirs:\n",
    "        run_name = os.path.basename(run_dir)\n",
    "\n",
    "        json_files = [f for f in os.listdir(run_dir) if f.endswith(\".json\")]\n",
    "        if not json_files:\n",
    "            print(f\"[WARN] No json found in {run_dir}\")\n",
    "            continue\n",
    "\n",
    "        json_path = os.path.join(run_dir, json_files[0])\n",
    "        print(f\"Loading {json_path}\")\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        for report in data.get(\"competition_reports\", []):\n",
    "            comp = report.get(\"competition_id\")\n",
    "            if comp is None:\n",
    "                continue\n",
    "\n",
    "            if comp in evo_list:\n",
    "                rows.append({\n",
    "                    \"run\": run_name,\n",
    "                    \"competition\": comp,\n",
    "                    \"score\": report.get(\"score\"),\n",
    "                })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    if df.empty:\n",
    "        raise RuntimeError(\"No EVO competitions found.\")\n",
    "\n",
    "    # ✅ 这里已经是 3 次 run 的 mean / std\n",
    "    df_sota = (\n",
    "        df.groupby(\"competition\")[\"score\"]\n",
    "          .agg(mean=\"mean\", std=\"std\")\n",
    "          .reset_index()\n",
    "    )\n",
    "\n",
    "    df_sota[\"experiment\"] = experiment_name\n",
    "    return df_sota\n",
    "\n",
    "\n",
    "RUN_DIRS1 = [ \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/rdagent_group9\",\n",
    " \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/rdagent_group10\",\n",
    "   \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/rdagent_group11\", ]\n",
    "\n",
    "\n",
    "\n",
    "df_sota1 = build_sota_df(\n",
    "    RUN_DIRS1,\n",
    "    EVO,\n",
    "    experiment_name=\"rd-agent-sota\"\n",
    ")\n",
    "\n",
    "RUN_DIRS2 = [ \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/aira-dojo_group18\",\n",
    " \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/aira-dojo_group19\",\n",
    "   \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/aira-dojo_group20\", ]\n",
    "\n",
    "# df_sota2 = build_sota_df(\n",
    "#     RUN_DIRS2,\n",
    "#     EVO,\n",
    "#     experiment_name=\"aira-dojo\"\n",
    "# )\n",
    "\n",
    "# RUN_DIRS3 = [ \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/internagent_group1\",\n",
    "#  \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/internagent_group2\",\n",
    "#    \"/data/userdata/v-lijingyuan/mle_score/mle-bench/runs/internagent_group3\", ]\n",
    "\n",
    "# df_sota3 = build_sota_df(\n",
    "#     RUN_DIRS3,\n",
    "#     EVO,\n",
    "#     experiment_name=\"Intern-agent\"\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56873309",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.concat(\n",
    "    [df_base, df_sota1],#,df_sota2,df_sota3],\n",
    "    ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c225e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_map = {\n",
    "    \"smooth-tuna\": \"RD-Agent-GPT5-baseline\",\n",
    "    \"quality-cougar\": \"RD-Agent-GPT5-world-model\",\n",
    "    \"rd-agent-sota\":\"RD-Agent-GPT5-SOTA\",\n",
    "    # \"aira-dojo\":\"Aira-dojo\"\n",
    "}\n",
    "df_1[\"experiment\"] = df_1[\"experiment\"].replace(name_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b848cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df_1.sort_values(by=\"competition\").reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccce4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.to_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93539093",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def add_rank(g):\n",
    "    # 同一个 competition 里 direction 是一致的\n",
    "    direction = g[\"direction\"].iloc[0]\n",
    "\n",
    "    ascending = True if direction == \"↓\" else False\n",
    "\n",
    "    g = g.sort_values(\"mean\", ascending=ascending)\n",
    "    g[\"rank\"] = np.arange(1, len(g) + 1)\n",
    "    return g\n",
    "\n",
    "df_ranked = (\n",
    "    df_1\n",
    "    .groupby(\"competition\", group_keys=False)\n",
    "    .apply(add_rank)\n",
    ")\n",
    "top_stats = (\n",
    "    df_ranked\n",
    "    .assign(\n",
    "        top1=lambda x: x[\"rank\"] == 1,\n",
    "        top2=lambda x: x[\"rank\"] <= 2,\n",
    "    )\n",
    "    .groupby(\"experiment\")[[\"top1\", \"top2\"]]\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb4f678",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467f9f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_is_better = {\n",
    "    \"leaf-classification\",\n",
    "    \"spooky-author-identification\",\n",
    "}\n",
    "target_experiments = {\n",
    "    \"RD-Agent-GPT5-world-model\",\n",
    "    \"RD-Agent-GPT5-baseline\",\n",
    "    \"RD-Agent-GPT5-SOTA\",\n",
    "}\n",
    "\n",
    "df = df_1[df_1[\"experiment\"].isin(target_experiments)].copy()\n",
    "def pick_top2(g):\n",
    "    comp = g[\"competition\"].iloc[0]\n",
    "    ascending = comp in lower_is_better  # ↓ 任务\n",
    "\n",
    "    g = g.sort_values(\"mean\", ascending=ascending)\n",
    "\n",
    "    g[\"rank\"] = [1, 2, 3]  # 固定只有 3 个\n",
    "    return g.iloc[:2]      # 只保留 top1 / top2\n",
    "df_top = (\n",
    "    df.groupby(\"competition\", group_keys=False)\n",
    "      .apply(pick_top2)\n",
    ")\n",
    "\n",
    "summary = (\n",
    "    df_top.groupby([\"experiment\", \"rank\"])\n",
    "          .size()\n",
    "          .unstack(fill_value=0)\n",
    "          .rename(columns={1: \"top1\", 2: \"top2\"})\n",
    "          .reset_index()\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735c6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6494ac88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_latex_table(df, higher_is_better=True, digits=4):\n",
    "    lines = []\n",
    "\n",
    "    lines.append(r\"\\begin{table}[t]\")\n",
    "    lines.append(r\"\\centering\")\n",
    "    lines.append(r\"\\resizebox{\\linewidth}{!}{\")\n",
    "    lines.append(r\"\\begin{tabular}{l l c}\")\n",
    "    lines.append(r\"\\hline\")\n",
    "    lines.append(r\"\\textbf{Competition ID} (MLE-Bench) & \\textbf{Method} & \\textbf{Score$\\pm$std} \\\\\")\n",
    "    lines.append(r\"\\hline\")\n",
    "\n",
    "    for comp, g in df.groupby(\"competition\", sort=True):\n",
    "        g = g.copy()\n",
    "\n",
    "        # 找 best\n",
    "        if higher_is_better:\n",
    "            best_idx = g[\"mean\"].idxmax()\n",
    "        else:\n",
    "            best_idx = g[\"mean\"].idxmin()\n",
    "\n",
    "        first = True\n",
    "        for idx, row in g.iterrows():\n",
    "            mean = row[\"mean\"]\n",
    "            std = row[\"std\"]\n",
    "\n",
    "            score_str = f\"{mean:.{digits}f} \\\\pm {std:.{digits}f}\"\n",
    "            if idx == best_idx:\n",
    "                score_str = rf\"\\textbf{{{mean:.{digits}f}}} \\pm {std:.{digits}f}\"\n",
    "\n",
    "            if first:\n",
    "                lines.append(\n",
    "                    rf\"{comp} ($\\uparrow$) & {row['experiment']} & ${score_str}$ \\\\\"\n",
    "                )\n",
    "                first = False\n",
    "            else:\n",
    "                lines.append(\n",
    "                    rf\" & {row['experiment']} & ${score_str}$ \\\\\"\n",
    "                )\n",
    "\n",
    "        lines.append(r\"\\hline\")\n",
    "\n",
    "    lines.append(r\"\\end{tabular}\")\n",
    "    lines.append(r\"}\")\n",
    "    lines.append(r\"\\end{table}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6caeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "latex_str = df_to_latex_table(df_1)\n",
    "print(latex_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
