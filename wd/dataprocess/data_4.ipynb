{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "from transformers import AutoTokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efef55d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Thinking-2507\")\n",
    "\n",
    "amlt_root_json_path = Path(\"/home/bowen/workspace/fine-tune/root_id_map.json\")\n",
    "#for exp_json in amlt_json_path.iterdir()[0:3]:\n",
    "with open(amlt_root_json_path, \"r\") as f:\n",
    "    root_map = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b218a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hypo_chain(parent_hyps, current_hyp, max_tokens=8000):\n",
    "    \"\"\"\n",
    "    拼接 parent_hyps （按时间顺序），加入 current_hyp，截断到 max_tokens\n",
    "    \"\"\"\n",
    "    sep = \"->\"\n",
    "    chain = parent_hyps[::-1] + [current_hyp]  # parent从早到晚 + 当前\n",
    "\n",
    "    while True:\n",
    "        chain_text = sep.join(chain)\n",
    "        tokenized = tokenizer(chain_text, add_special_tokens=False)\n",
    "        tokens = len(tokenized[\"input_ids\"])\n",
    "\n",
    "        # ✅ 已满足长度\n",
    "        if tokens <= max_tokens:\n",
    "            return chain_text\n",
    "\n",
    "        # ✅ 截断最前面的 parent hypothesis\n",
    "        if len(chain) > 1:\n",
    "            chain.pop(0)\n",
    "            continue\n",
    "\n",
    "        # ✅ 到这说明只剩 current_hyp 还超 → 硬截断 current_hyp 尾部\n",
    "        truncated_ids = tokenized[\"input_ids\"][-max_tokens:]\n",
    "        return tokenizer.decode(truncated_ids, skip_special_tokens=True)\n",
    "\n",
    "        \n",
    "        \n",
    "def get_parent_hypotheses(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    hypotheses = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            hypotheses.append(parent_entry['input']['hypothesis'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return hypotheses\n",
    "\n",
    "\n",
    "def get_parent_scores(id_to_entry,entry):\n",
    "    \"\"\"递归获取父条目的 hypothesis\"\"\"\n",
    "    scores = []\n",
    "    parent_id = entry['input'].get('parent_id')\n",
    "    while parent_id:\n",
    "        parent_entry = id_to_entry.get(parent_id)\n",
    "        if not parent_entry:\n",
    "            break\n",
    "        if parent_entry['input'][\"feedback_decision\"] == True:\n",
    "            scores.append(parent_entry['input']['valid_score'])\n",
    "        parent_id = parent_entry['input'].get('parent_id')\n",
    "    return scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddadec89",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pairs = []\n",
    "amlt_json_path = Path(\"/home/bowen/workspace/fine-tune/amlt_jsons\")\n",
    "for exp_json in list(amlt_json_path.iterdir())[0:1]:\n",
    "    with open(exp_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    for ids, loop_data in data.items():\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aff3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39a2299",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = []\n",
    "final_pairs = []\n",
    "amlt_json_path = Path(\"/home/bowen/workspace/fine-tune/amlt_jsons\")\n",
    "#for exp_json in amlt_json_path.iterdir()[0:3]:\n",
    "#for exp_json in list(amlt_json_path.iterdir()):\n",
    "for exp_json in list(amlt_json_path.iterdir())[0:3]:\n",
    "    with open(exp_json, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    all_pairs = []\n",
    "    for ids, loop_data in data.items():\n",
    "        comptation_name = ids.split(\" \")[1]\n",
    "        if ids.split(\" \")[-1]== \"scenario\":\n",
    "            bigger_is_better = int(loop_data['metric_direction'])\n",
    "\n",
    "        if \"final_hypothesis\" in loop_data and \"feedback\" in loop_data and \"code\" in loop_data:\n",
    "            first_metric = next(iter(loop_data[\"valid_score\"].values()))\n",
    "            alpaca_data = {\n",
    "                    \"input\": {\n",
    "                        \"exp_name\": exp_json.name.replace(\".json\", \"\"),\n",
    "                        \"comptation_name\":comptation_name,\n",
    "                        \"bigger_is_better\": bigger_is_better,\n",
    "                        \"loop_id\": int(ids.split(\" \")[-1]),\n",
    "                        \"hypothesis\": loop_data[\"final_hypothesis\"][\"hypothesis\"],\n",
    "                        #\"test_report\": loop_data[\"test_report\"][\"score\"],\n",
    "                        \"valid_score\": first_metric.get(\"ensemble\", None),\n",
    "                        \"feedback_decision\": loop_data[\"feedback\"]['decision'],\n",
    "                        \"parent_id\": loop_data.get(\"parent_id\", None) ,\n",
    "                        \"root_id\" : int(root_map[ids].split(\" \")[-1]) if ids in root_map else None\n",
    "                    }\n",
    "                }\n",
    "            all_pairs.append(alpaca_data)\n",
    "\n",
    "    all_pairs_new = []\n",
    "    id_to_entry = {}\n",
    "    for entry in all_pairs:\n",
    "        key = f\"{entry['input']['exp_name']} {entry['input']['comptation_name']} {entry['input']['loop_id']}\"\n",
    "        id_to_entry[key] = entry\n",
    "\n",
    "    id_to_entry1 = {}\n",
    "    for entry in all_pairs:\n",
    "        key = (entry['input']['exp_name'], entry['input']['comptation_name'], int(entry['input']['loop_id']))\n",
    "        id_to_entry1[key] = entry\n",
    "\n",
    "\n",
    "    for target_entry in all_pairs:\n",
    "        parent_hyps = get_parent_hypotheses(id_to_entry,target_entry)\n",
    "        target_entry['input']['hypothesis_chain'] =build_hypo_chain(parent_hyps, target_entry['input'].get('hypothesis'))#\"<think_step>\".join(parent_hyps[::-1] + [target_entry['input'].get('hypothesis')] )\n",
    "        parnet_scores = get_parent_scores(id_to_entry,target_entry)\n",
    "        if len(parnet_scores)>0: \n",
    "            target_entry['input']['parent_score'] = parnet_scores[0]\n",
    "        else:\n",
    "            target_entry['input']['parent_score'] = 10000000\n",
    "\n",
    "        all_pairs_new.append(target_entry)\n",
    "\n",
    "    del all_pairs\n",
    "    final_pairs.extend(all_pairs_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585bab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pairs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
