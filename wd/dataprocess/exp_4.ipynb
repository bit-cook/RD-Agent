{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66d73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "from typing import cast\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_json(log_content: str) -> dict | None:\n",
    "    match = re.search(r\"\\{.*\\}\", log_content, re.DOTALL)\n",
    "    if match:\n",
    "        return cast(dict, json.loads(match.group(0)))\n",
    "    return None\n",
    "\n",
    "\n",
    "amlt_path = Path(\"/data/share_folder_local/amlt\")\n",
    "\n",
    "amlt_names = [\n",
    "    \"adjusted-phoenix\",\n",
    "    \"mighty-whippet\",\n",
    "    \"devoted-louse\",\n",
    "    \"warm-tahr\"\n",
    "]\n",
    "\n",
    "records = []\n",
    "\n",
    "for amlt_name in amlt_names:\n",
    "    amlt_dir = amlt_path / amlt_name\n",
    "\n",
    "    for competition_dir in amlt_dir.iterdir():\n",
    "        if not competition_dir.is_dir():\n",
    "            continue\n",
    "\n",
    "        grade_file = competition_dir / \"grade.log\"\n",
    "        if not grade_file.exists():\n",
    "            continue\n",
    "\n",
    "        grade_data = extract_json(grade_file.read_text())\n",
    "        if grade_data is None:\n",
    "            continue\n",
    "\n",
    "        records.append({\n",
    "            \"amlt_name\": amlt_name,\n",
    "            \"experiment_name\": competition_dir.name,\n",
    "            \"competition_id\": grade_data.get(\"competition_id\"),\n",
    "            \"score\": grade_data.get(\"score\"),\n",
    "            \"any_medal\": grade_data.get(\"any_medal\", False),\n",
    "        })\n",
    "\n",
    "# ==== Âª∫ dataframe ====\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# üëâ ‰∏∫Âêå‰∏Ä‰∏™ (competition_id, amlt_name) ÁöÑÈáçÂ§çÂÆûÈ™åÁºñÂè∑\n",
    "df = df.sort_values(\n",
    "    [\"competition_id\", \"amlt_name\", \"experiment_name\"]\n",
    ")\n",
    "\n",
    "df[\"run_id\"] = (\n",
    "    df.groupby([\"competition_id\", \"amlt_name\"])\n",
    "      .cumcount()\n",
    "      .add(1)\n",
    ")\n",
    "\n",
    "df[\"score_col\"] = (\n",
    "    df[\"amlt_name\"] + \"_score_\" + df[\"run_id\"].astype(str)\n",
    ")\n",
    "df[\"medal_col\"] = (\n",
    "    df[\"amlt_name\"] + \"_medal_\" + df[\"run_id\"].astype(str)\n",
    ")\n",
    "\n",
    "# df[\"score_col\"] = df[\"amlt_name\"] + \"_score_1\"\n",
    "# df[\"medal_col\"] = df[\"amlt_name\"] + \"_medal_1\"\n",
    "\n",
    "\n",
    "# ==== pivotÔºà‰∏çÂÜçÂêàÂπ∂ÈáçÂ§çÂÆûÈ™åÔºâ====\n",
    "score_wide = df.pivot(\n",
    "    index=\"competition_id\",\n",
    "    columns=\"score_col\",\n",
    "    values=\"score\"\n",
    ")\n",
    "\n",
    "medal_wide = df.pivot(\n",
    "    index=\"competition_id\",\n",
    "    columns=\"medal_col\",\n",
    "    values=\"any_medal\"\n",
    ")\n",
    "\n",
    "# ==== ÂêàÂπ∂ ====\n",
    "result = (\n",
    "    pd.concat([score_wide, medal_wide], axis=1)\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result.drop(\n",
    "    columns=[\"adjusted-phoenix_score_2\", \"adjusted-phoenix_score_1\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5181a515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(\"/data/userdata/v-lijingyuan/RD-agent-mcts/RD-Agent/.env\", override=True)\n",
    "\n",
    "# È™åËØÅ\n",
    "print(os.getenv(\"CHAT_TEMPERATURE\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c89adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "PICKLE_CACHE_FOLDER_PATH_STR=\"/home/bowen/RD-Agent_cache\"\n",
    "from rdagent.scenarios.kaggle.kaggle_crawler import get_metric_direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcd7c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for comp in result[\"competition_id\"]:\n",
    "#     direction = get_metric_direction(comp)\n",
    "#     print(f\"Competition ID: {comp}, Metric Direction: {direction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481dece2",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cols = [c for c in result.columns if \"_score_\" in c]\n",
    "\n",
    "def highlight_best(row):\n",
    "    comp = row[\"competition_id\"]\n",
    "    higher_is_better = get_metric_direction(comp)  # True / False\n",
    "\n",
    "    values = row[score_cols]\n",
    "\n",
    "    if values.isna().all():\n",
    "        return [\"\"] * len(row)\n",
    "\n",
    "    best_val = values.max() if higher_is_better else values.min()\n",
    "\n",
    "    return [\n",
    "        \"font-weight: bold\"\n",
    "        if col in score_cols and row[col] == best_val\n",
    "        else \"\"\n",
    "        for col in row.index\n",
    "    ]\n",
    "\n",
    "styled = result.style.apply(highlight_best, axis=1)\n",
    "styled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6e1f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_cols(row, k=1):\n",
    "    comp = row[\"competition_id\"]\n",
    "    higher_is_better = get_metric_direction(comp)\n",
    "\n",
    "    values = row[score_cols].dropna()\n",
    "    if values.empty:\n",
    "        return []\n",
    "\n",
    "    # ÊéíÂ∫è\n",
    "    sorted_vals = values.sort_values(ascending=not higher_is_better)\n",
    "\n",
    "    # distinct ÂÄº\n",
    "    distinct_vals = sorted_vals.unique()\n",
    "\n",
    "    if len(distinct_vals) < k:\n",
    "        return []\n",
    "\n",
    "    target_val = distinct_vals[k - 1]\n",
    "    return [c for c in score_cols if row[c] == target_val]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "top1_counter = Counter()\n",
    "top2_counter = Counter()\n",
    "\n",
    "for _, row in result.iterrows():\n",
    "    for c in get_topk_cols(row, k=1):\n",
    "        top1_counter[c] += 1\n",
    "\n",
    "    for c in get_topk_cols(row, k=2):\n",
    "        top2_counter[c] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42a7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranks(row):\n",
    "    comp = row[\"competition_id\"]\n",
    "    higher_is_better = get_metric_direction(comp)\n",
    "\n",
    "    values = row[score_cols]\n",
    "    ranks = values.rank(\n",
    "        ascending=not higher_is_better,\n",
    "        method=\"min\"\n",
    "    )\n",
    "    return ranks\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "top1 = Counter()\n",
    "top2 = Counter()\n",
    "rank_sum = defaultdict(float)\n",
    "rank_cnt = defaultdict(int)\n",
    "\n",
    "for _, row in result.iterrows():\n",
    "    # Top-k\n",
    "    for c in get_topk_cols(row, k=1):\n",
    "        top1[c] += 1\n",
    "    for c in get_topk_cols(row, k=2):\n",
    "        top2[c] += 1\n",
    "\n",
    "    # Rank\n",
    "    ranks = get_ranks(row)\n",
    "    for c, r in ranks.items():\n",
    "        if not np.isnan(r):\n",
    "            rank_sum[c] += r\n",
    "            rank_cnt[c] += 1\n",
    "import pandas as pd\n",
    "\n",
    "methods = score_cols\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"method\": methods,\n",
    "    \"top1_count\": [top1[m] for m in methods],\n",
    "    \"top2_count\": [top2[m] for m in methods],\n",
    "    \"top12_count\": [top1[m] + top2[m] for m in methods],\n",
    "    \"avg_rank\": [\n",
    "        rank_sum[m] / rank_cnt[m] if rank_cnt[m] > 0 else float(\"nan\")\n",
    "        for m in methods\n",
    "    ],\n",
    "})\n",
    "\n",
    "df = df.sort_values(\n",
    "    by=[\"top12_count\", \"top1_count\", \"avg_rank\"],\n",
    "    ascending=[False, False, True]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711e464e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2535c8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path = \"/data/userdata/v-lijingyuan/icml2026/ml-master.csv\"\n",
    "result.to_csv(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277f89a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"devoted-louse_score_1\"\n",
    "b = \"mighty-whippet_score_1\"\n",
    "\n",
    "cmp = result[[\"competition_id\", a, b]].dropna()\n",
    "\n",
    "def devoted_louse_win(row):\n",
    "    higher_is_better = get_metric_direction(row[\"competition_id\"])\n",
    "    if higher_is_better:\n",
    "        return row[a] > row[b]\n",
    "    else:\n",
    "        return row[a] < row[b]\n",
    "\n",
    "def mighty_whippet_win(row):\n",
    "    higher_is_better = get_metric_direction(row[\"competition_id\"])\n",
    "    if higher_is_better:\n",
    "        return row[b] > row[a]\n",
    "    else:\n",
    "        return row[b] < row[a]\n",
    "\n",
    "stats = {\n",
    "    \"devoted_louse_win\": cmp.apply(devoted_louse_win, axis=1).sum(),\n",
    "    \"mighty_whippet_win\": cmp.apply(mighty_whippet_win, axis=1).sum(),\n",
    "    \"tie\": (cmp[a] == cmp[b]).sum(),\n",
    "    \"total\": len(cmp),\n",
    "}\n",
    "\n",
    "stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a08a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward = [\"of custom CNNs, I will use transfer learning from a high-performing pre-trained model designed for small images: EfficientNet-b0 from the `timm` library, finetuned for binary classification. I'll freeze most layers and train only the final classifier head to prevent overfitting, given the relatively small dataset. During training, I'll use moderate data augmentation to improve generalization, perform a stratified train/validation split, and report ROC AUC for the validation set. I'll save the test predictions as required in `./submission/submission.csv`, with each row containing the image id and the model-predicted probability for the cactus class.\", \"of using a CNN built from scratch as in previous attempts, I propose using transfer learning with a lightweight pretrained architecture (such as MobileNetV2) from torchvision. We'll adapt the model by fine-tuning only the classifier head on the 32x32 images, applying simple augmentations (random horizontal/vertical flips) to reduce overfitting. We'll use a validation split to monitor ROC-AUC, saving the best model, and output test probabilities to `submission/submission.csv`. This approach leverages the strong feature extractors of pretrained models, which should boost performance compared to training from scratch.\", \"of using a custom CNN or a classifier built from scratch, I will leverage transfer learning by fine-tuning a lightweight pretrained CNN from timm (such as EfficientNet-B0) for this small image binary classification task. Even though the input images are only 32x32, pretrained models can still help by capturing robust low-level features. I'll resize all images to fit the model's expected input size, replace the classifier head for binary output, apply basic augmentations, and use early stopping on a validation ROC AUC. After training, I'll predict probabilities for the test images and write them to `./submission/submission.csv`. The ROC AUC on a validation set will be printed after training.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_for_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbd213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57d3d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward = ['implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model\\'s out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We\\'ll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->her improve the existing stacked model (TF-IDF + Logistic Regression + LGBM) without major architectural change, I will add character n-gram features to the TF-IDF vectorizer in addition to word n-grams. Character n-grams are particularly effective for toxicity detection, as they can capture variations in offensive language and spelling that word n-grams may miss. Specifically, I will set `analyzer=\"char_wb\"` with n-gram range (3,5) for a second vectorizer, concatenate its features with the word-based TF-IDF, and proceed as before. The code below includes this change, produces validation metrics, and outputs the required submission file.->ove the previous solution, I propose to add text preprocessing/cleaning before feature extraction, specifically by removing special characters, unnecessary white spaces, and converting all text to lowercase. This standardization can improve the quality of the TF-IDF features, potentially resulting in improved model performance. The rest of the modeling pipeline will remain unchanged, so we can directly observe the impact of preprocessing on validation ROC-AUC. The new cleaning step will be applied to all train, validation, and test comment texts before TF-IDF vectorization.->ove the previous stacked TF-IDF solution, I propose to add feature enrichment using a pretrained transformer-based sentence embedding model (specifically, `sentence-transformers/all-MiniLM-L6-v2`). We\\'ll extract dense vector representations for each comment and concatenate these embeddings to the existing TF-IDF features. The rationale is that transformer-based semantic features should complement ngram-based models and help with generalization when stacked with the existing pipeline. The approach will be implemented for both training and test data, used in both level-1 (LogisticRegression) and level-2 (LightGBM) models, and evaluated via mean column-wise ROC-AUC on the validation set. The final test predictions will be saved to `./submission/submission.csv`.', 'implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model\\'s out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We\\'ll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->her improve the existing stacked model (TF-IDF + Logistic Regression + LGBM) without major architectural change, I will add character n-gram features to the TF-IDF vectorizer in addition to word n-grams. Character n-grams are particularly effective for toxicity detection, as they can capture variations in offensive language and spelling that word n-grams may miss. Specifically, I will set `analyzer=\"char_wb\"` with n-gram range (3,5) for a second vectorizer, concatenate its features with the word-based TF-IDF, and proceed as before. The code below includes this change, produces validation metrics, and outputs the required submission file.->ove the previous solution, I propose to add text preprocessing/cleaning before feature extraction, specifically by removing special characters, unnecessary white spaces, and converting all text to lowercase. This standardization can improve the quality of the TF-IDF features, potentially resulting in improved model performance. The rest of the modeling pipeline will remain unchanged, so we can directly observe the impact of preprocessing on validation ROC-AUC. The new cleaning step will be applied to all train, validation, and test comment texts before TF-IDF vectorization.->ove upon the previous solution, I propose the following: The previous approach stacked Logistic Regression (Level 1) predictions using LightGBM as a meta-model, but it did not utilize domain-specific features beyond TF-IDF. As an actionable next step, I will engineer several hand-crafted features derived from the raw text (such as comment length, number of capital letters, number of exclamation/question marks, count of unique words, proportion of uppercase, etc.). These features will be concatenated with the TF-IDF features, and the same pipeline will be used (Logistic Regression + LightGBM stack). This improvement is expected to help especially with short/loud/shouty or ‚Äúobvious‚Äù toxic comments and should provide an incremental boost in validation ROC-AUC.', 'implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model\\'s out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We\\'ll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->her improve the existing stacked model (TF-IDF + Logistic Regression + LGBM) without major architectural change, I will add character n-gram features to the TF-IDF vectorizer in addition to word n-grams. Character n-grams are particularly effective for toxicity detection, as they can capture variations in offensive language and spelling that word n-grams may miss. Specifically, I will set `analyzer=\"char_wb\"` with n-gram range (3,5) for a second vectorizer, concatenate its features with the word-based TF-IDF, and proceed as before. The code below includes this change, produces validation metrics, and outputs the required submission file.->ove the previous solution, I propose to add text preprocessing/cleaning before feature extraction, specifically by removing special characters, unnecessary white spaces, and converting all text to lowercase. This standardization can improve the quality of the TF-IDF features, potentially resulting in improved model performance. The rest of the modeling pipeline will remain unchanged, so we can directly observe the impact of preprocessing on validation ROC-AUC. The new cleaning step will be applied to all train, validation, and test comment texts before TF-IDF vectorization.->vious solution uses TF-IDF features and stacking (LogisticRegression + LightGBM). An actionable improvement is to enrich the model by adding pretrained transformer embeddings (e.g., MiniLM) as additional features, concatenated to the TF-IDF representation. This provides semantic information the classic TF-IDF cannot capture, and could improve accuracy especially on subtle examples. I will use `sentence-transformers` to extract MiniLM embeddings, concatenate with TF-IDF, then rerun the same stacking architecture. This preserves the validation/test pipeline and submission logic.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a83413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward = ['implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model\\'s out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We\\'ll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->her improve the existing stacked model (TF-IDF + Logistic Regression + LGBM) without major architectural change, I will add character n-gram features to the TF-IDF vectorizer in addition to word n-grams. Character n-grams are particularly effective for toxicity detection, as they can capture variations in offensive language and spelling that word n-grams may miss. Specifically, I will set `analyzer=\"char_wb\"` with n-gram range (3,5) for a second vectorizer, concatenate its features with the word-based TF-IDF, and proceed as before. The code below includes this change, produces validation metrics, and outputs the required submission file.', \"implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model's out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We'll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->vious solution uses a Logistic Regression + LightGBM stacking pipeline with only TF-IDF-based meta-features for the stacker. A simple, atomic improvement is to augment the meta-feature stack: For the LightGBM (level-2) model, in addition to the Logistic Regression OOF predictions, supply a few hand-crafted meta-features derived from the comments themselves‚Äîsuch as length of the comment, number of capital letters, number of exclamation/question marks, and ratio of punctuation. This will provide additional signals for the stacking model, potentially enhancing discrimination and mean ROC-AUC. All other parts (CV, vectorization, stacking) are unchanged for isolation and clarity of effect.\", \"implement a straightforward text classification pipeline using TF-IDF features and a Logistic Regression classifier for each toxicity target (multi-label, six heads). Each classifier will independently predict the probability for one label, and the final performance will be evaluated using mean column-wise ROC-AUC on a hold-out validation set from the training data. After validation, we will use the entire training set to retrain the classifiers and produce test set predictions. The submission CSV will be saved at `./submission/submission.csv`.->ove the previous solution, I propose stacking a second-level non-linear model (e.g., a LightGBM classifier) on top of the LogisticRegression base model's out-of-fold predictions. The first step is to generate out-of-fold predictions on the training set and predictions on the validation set for each target using the existing TF-IDF+LR pipeline. Then, these predictions (together with the original TF-IDF features, if desired) are used as input to train LightGBM OneVsRest classifiers. This stacking introduces nonlinearity, can leverage differing strengths of the models, and often boosts ROC-AUC in multilabel settings. We'll report the mean column-wise ROC-AUC on the validation set and output the test submission CSV as before.->her improve the stacking ensemble, I propose to extend the input to the Level-2 (LightGBM) model by concatenating both the original TF-IDF features and the out-of-fold Logistic Regression (Level-1) prediction probabilities. This gives LightGBM richer information and lets it use high-dimensional text features alongside meta-predictions, increasing its modeling capacity while retaining interpretability. The code below modifies the stacking process accordingly and prints the validation ROC-AUC. The submission file is written to the required location.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d335a4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(texts_for_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a8d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward[0].split(\"->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03cbbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward[1].split(\"->\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260a4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_reward[2].split(\"->\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
